{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Tweet   \n",
      "0                      @user @user @user Probably true  \\\n",
      "1            @user Major update coming in a few months   \n",
      "2    RT @user Starship is the largest, most capable...   \n",
      "3    @user Then he added ‚ÄúSilicon is for n00bs, Gal...   \n",
      "4                     @user @user @user @user @user üòÇüíØ   \n",
      "..                                                 ...   \n",
      "345                       @user Thanks for the 3 Doge!   \n",
      "346  @user True, electronics all want DC, not AC. S...   \n",
      "347                   @user Welcome back to Twitter! üíï   \n",
      "348                    May the 4th be with you ‚ù§Ô∏è http   \n",
      "349  @user @user Yes, but they are not to blame for...   \n",
      "\n",
      "                         Time  \n",
      "0   2023-05-11 03:11:16-07:00  \n",
      "1   2023-05-11 03:08:59-07:00  \n",
      "2   2023-05-11 02:32:11-07:00  \n",
      "3   2023-05-11 02:24:48-07:00  \n",
      "4   2023-05-11 02:17:47-07:00  \n",
      "..                        ...  \n",
      "345 2023-05-04 11:04:34-07:00  \n",
      "346 2023-05-04 11:01:57-07:00  \n",
      "347 2023-05-04 10:58:59-07:00  \n",
      "348 2023-05-04 10:56:08-07:00  \n",
      "349 2023-05-04 10:48:32-07:00  \n",
      "\n",
      "[350 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('2musk_tweets.csv', encoding='utf-8')\n",
    "\n",
    "# Append the row entries to an array\n",
    "data_array = df.values.tolist()\n",
    "\n",
    "# Define the processing function\n",
    "def process_tweet(tweet):\n",
    "    if isinstance(tweet, float):\n",
    "        return \"\"\n",
    "    tweet_words = []\n",
    "    for word in tweet.split(' '):\n",
    "        if word.startswith('@') and len(word) > 1:\n",
    "            word = '@user'\n",
    "        elif word.startswith('http'):\n",
    "            word = \"http\"\n",
    "        tweet_words.append(word)\n",
    "    tweet_processed = \" \".join(tweet_words)\n",
    "    return tweet_processed\n",
    "\n",
    "# Apply the processing function to the 'Tweet' column and store the results in a new column\n",
    "df['Tweet'] = df['Tweet'].apply(process_tweet)\n",
    "\n",
    "# Convert 'Time (+0)' column to datetime type\n",
    "df['Time (+0)'] = pd.to_datetime(df['Time (+0)'])\n",
    "\n",
    "# Formatting\n",
    "current_timezone = pytz.timezone('UTC')\n",
    "df['Time (US/Pacific)'] = df['Time (+0)'].dt.tz_localize(current_timezone).dt.tz_convert('US/Pacific')\n",
    "df = df.drop(columns=['Time (+0)'])\n",
    "df = df.rename(columns={'Time (US/Pacific)': 'Time'})\n",
    "\n",
    "print(df)\n",
    "df.to_csv('3processed_tweets.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Time (+0)                                              Tweet   \n",
      "0    2023-05-11 10:11:16                    @user @user @user Probably true  \\\n",
      "1    2023-05-11 10:08:59          @user Major update coming in a few months   \n",
      "2    2023-05-11 09:32:11  RT @user Starship is the largest, most capable...   \n",
      "3    2023-05-11 09:24:48  @user Then he added ‚ÄúSilicon is for n00bs, Gal...   \n",
      "4    2023-05-11 09:17:47                   @user @user @user @user @user üòÇüíØ   \n",
      "..                   ...                                                ...   \n",
      "345  2023-05-04 18:04:34                       @user Thanks for the 3 Doge!   \n",
      "346  2023-05-04 18:01:57  @user True, electronics all want DC, not AC. S...   \n",
      "347  2023-05-04 17:58:59                   @user Welcome back to Twitter! üíï   \n",
      "348  2023-05-04 17:56:08                    May the 4th be with you ‚ù§Ô∏è http   \n",
      "349  2023-05-04 17:48:32  @user @user Yes, but they are not to blame for...   \n",
      "\n",
      "    Sentiment Negative Score Neutral Score Positive Score  \n",
      "0     Neutral        0.01171      0.920015       0.068275  \n",
      "1     Neutral       0.004564      0.715443       0.279993  \n",
      "2    Positive       0.001439      0.005551        0.99301  \n",
      "3     Neutral       0.006513      0.964699       0.028788  \n",
      "4     Neutral       0.010035      0.541732       0.448233  \n",
      "..        ...            ...           ...            ...  \n",
      "345  Positive       0.001509      0.006189       0.992302  \n",
      "346   Neutral       0.022207      0.683745       0.294048  \n",
      "347  Positive       0.001325      0.009529       0.989146  \n",
      "348  Positive       0.001573      0.006058        0.99237  \n",
      "349  Negative       0.844547      0.140118       0.015335  \n",
      "\n",
      "[350 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "roberta = \"cardiffnlp/roberta-base-tweet-sentiment-en\" # https://huggingface.co/cardiffnlp/roberta-base-tweet-sentiment-en/tree/main\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "df['Sentiment'] = ''\n",
    "df['Negative Score'] = ''\n",
    "df['Neutral Score'] = ''\n",
    "df['Positive Score'] = ''\n",
    "\n",
    "# Perform sentiment analysis\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    encoded_tweet = tokenizer(tweet, return_tensors='pt')\n",
    "    output = model(**encoded_tweet)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    df.at[index, 'Sentiment'] = labels[scores.argmax()]\n",
    "    df.at[index, 'Negative Score'] = scores[0]\n",
    "    df.at[index, 'Neutral Score'] = scores[1]\n",
    "    df.at[index, 'Positive Score'] = scores[2]\n",
    "\n",
    "# # Print sentiment labels and scores\n",
    "# for i in range(len(scores)):\n",
    "#     label = labels[i]\n",
    "#     score = scores[i]\n",
    "#     print(\"Tweet:\", tweet)\n",
    "#     print(\"Sentiment:\", label)\n",
    "#     print(\"Score:\", score)\n",
    "#     print()\n",
    "\n",
    "print(df)\n",
    "df.to_csv('4sentiment.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
